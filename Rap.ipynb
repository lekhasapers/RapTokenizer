{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU01F5ILxIGHH9W5Mf7r8K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEP1v7DtlMsm"
      },
      "outputs": [],
      "source": [
        "!pip install gensim nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('Lil Wayne_lyrics.txt', 'r', encoding='utf-8') as file:\n",
        "    wayne_text = file.read()\n",
        "\n",
        "# You might want to print a snippet to verify\n",
        "print(wayne_text[:500])  # Print the first 500 characters of the text\n"
      ],
      "metadata": {
        "id": "2HLu0rqbnYam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39ce1c1-63c7-4f31-aa4b-1e6267d73c34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿\n",
            "I'm on that good kush and alcohol\n",
            "I got some down bitches I can call\n",
            "I don't know what I would do without y'all\n",
            "I'ma ball 'til the day I fall\n",
            "\n",
            "Yeah, long as my bitches love me (yeah, yeah)\n",
            "I can give a fuck 'bout no hater\n",
            "Long as my bitches love me\n",
            "I can give a fuck 'bout no niggas\n",
            "Long as these bitches love me\n",
            "\n",
            "Uh, pussy-ass nigga, stop hatin'\n",
            "Lil Tunechi got that fire\n",
            "And these hoes love me like Satan, man\n",
            "Fuck with me and get bodied\n",
            "And all she eat is dick; she's on a strict diet\n",
            "That's my \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "nltk.download('punkt')  # Download necessary models for tokenization\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = sent_tokenize(wayne_text)\n",
        "\n",
        "# Tokenize each sentence into words\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "\n",
        "print(tokenized_sentences[0])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1aEiAZLZlctC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96972cd-82fa-49e1-812a-53c9da6f66e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeff', 'i', \"'m\", 'on', 'that', 'good', 'kush', 'and', 'alcohol', 'i', 'got', 'some', 'down', 'bitches', 'i', 'can', 'call', 'i', 'do', \"n't\", 'know', 'what', 'i', 'would', 'do', 'without', \"y'all\", \"i'ma\", 'ball', \"'til\", 'the', 'day', 'i', 'fall', 'yeah', ',', 'long', 'as', 'my', 'bitches', 'love', 'me', '(', 'yeah', ',', 'yeah', ')', 'i', 'can', 'give', 'a', 'fuck', \"'bout\", 'no', 'hater', 'long', 'as', 'my', 'bitches', 'love', 'me', 'i', 'can', 'give', 'a', 'fuck', \"'bout\", 'no', 'niggas', 'long', 'as', 'these', 'bitches', 'love', 'me', 'uh', ',', 'pussy-ass', 'nigga', ',', 'stop', \"hatin'\", 'lil', 'tunechi', 'got', 'that', 'fire', 'and', 'these', 'hoes', 'love', 'me', 'like', 'satan', ',', 'man', 'fuck', 'with', 'me', 'and', 'get', 'bodied', 'and', 'all', 'she', 'eat', 'is', 'dick', ';', 'she', \"'s\", 'on', 'a', 'strict', 'diet', 'that', \"'s\", 'my', 'baby', ',', 'with', 'no', 'makeup', 'she', 'a', 'ten', 'and', 'she', 'the', 'best', 'with', 'that', 'head', 'even', 'better', 'then', 'karrine', 'she', 'do', \"n't\", 'want', 'money', 'she', 'want', 'the', 'time', 'that', 'we', 'could', 'spend', 'she', 'said', ':', '``', '‘', 'cause', 'i', 'really', 'need', 'somebody', 'so', 'tell', 'me', 'you', \"'re\", 'that', 'somebody', '.', \"''\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Function to read and tokenize a text file\n",
        "def tokenize_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "        tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Initialize a list to hold all tokenized texts\n",
        "all_tokens = []\n",
        "\n",
        "# Iterate through the uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    tokens = tokenize_file(filename)\n",
        "    all_tokens.append(tokens)\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=all_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"rap_lyrics_word2vec.model\")\n",
        "\n",
        "# Save the word vectors in a format similar to GloVe\n",
        "output_file = \"glove_format_vectors.txt\"\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    # Write the header\n",
        "    vocab_size = len(model.wv)\n",
        "    vector_size = model.vector_size\n",
        "    f.write(f\"{vocab_size} {vector_size}\\n\")\n",
        "\n",
        "    # Write each word and its vector\n",
        "    for word in model.wv.index_to_key:\n",
        "        vector = model.wv[word]\n",
        "        vector_str = ' '.join(map(str, vector))\n",
        "        f.write(f\"{word} {vector_str}\\n\")\n",
        "\n",
        "# Download the file\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "96nCoti2KzA_",
        "outputId": "a827f2d0-7acf-4431-f190-1dafa3923f52"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec8a3907-92f6-4a87-afb4-f890ca8993e1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ec8a3907-92f6-4a87-afb4-f890ca8993e1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-62193284741a>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Upload files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initialize a list to hold all tokenized texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    157\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    158\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim pyphen\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwac-20xPwof",
        "outputId": "537bfe19-109d-48e4-d98b-1612c476ba9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Collecting pyphen\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Installing collected packages: pyphen\n",
            "Successfully installed pyphen-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Path to the text file containing word vectors\n",
        "file_path = 'vectorized_weezy_lyrics.txt'\n",
        "\n",
        "# Function to remove punctuation from words\n",
        "def clean_word(word):\n",
        "    return word.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Read the file and load the vectors, cleaning the words\n",
        "def load_vectors(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        # The first line contains the number of vectors and their dimensionality\n",
        "        num_vectors, vector_size = map(int, f.readline().split())\n",
        "        word_vectors = KeyedVectors(vector_size)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = clean_word(values[0])\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            word_vectors.add_vector(word, vector)\n",
        "\n",
        "    return word_vectors\n",
        "\n",
        "word_vectors = load_vectors(file_path)\n",
        "\n",
        "# Verify the vectors are loaded correctly\n",
        "#rint(f\"Loaded {len(word_vectors.key_to_index)} word vectors.\")\n",
        "\n",
        "# Function to fetch rhyming words using Datamuse API\n",
        "def get_rhyming_words(word, num_words=100):\n",
        "    url = f\"https://api.datamuse.com/words?trg_bga_rel={word}&max={num_words}\"\n",
        "    response = requests.get(url)\n",
        "    rhymes = response.json()\n",
        "    rhyming_words = [clean_word(rhyme['word']) for rhyme in rhymes if clean_word(rhyme['word']) in word_vectors.key_to_index]\n",
        "    return rhyming_words\n",
        "\n",
        "def generate_verse(start_word, num_lines=30, words_per_line=7):\n",
        "    verse = []\n",
        "    current_word = clean_word(start_word)\n",
        "    for _ in range(num_lines):\n",
        "        line = []\n",
        "        used_words = set()  # To keep track of words used in the current line\n",
        "        for _ in range(words_per_line - 1):\n",
        "            similar_words = word_vectors.most_similar(current_word, topn=50)\n",
        "            # Filter out words already used in the current line\n",
        "            filtered_words = [word for word, _ in similar_words if word not in used_words and word != current_word]\n",
        "            if not filtered_words:\n",
        "                break  # No more words to select, break out of the loop\n",
        "            next_word = random.choice(filtered_words)\n",
        "            line.append(next_word)\n",
        "            used_words.add(next_word)  # Add the selected word to the used set\n",
        "            current_word = next_word\n",
        "        rhyming_words = get_rhyming_words(current_word)\n",
        "        if rhyming_words:\n",
        "            line.append(random.choice(rhyming_words))\n",
        "        else:\n",
        "            line.append(current_word)\n",
        "        verse.append(\" \".join(line))\n",
        "    return \"\\n\".join(verse)\n",
        "\n",
        "start_word = \"money\"\n",
        "verse = generate_verse(start_word)\n",
        "print(verse)\n",
        "\n",
        "# Save the verse to a file to be used by ChucK\n",
        "with open(\"generated_verse.txt\", \"w\") as f:\n",
        "    f.write(verse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "y9naQbaSSRDd",
        "outputId": "d2d7704f-68c0-4482-bddc-665fe4dc8a35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'vectorized_weezy_lyrics.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0ffc2d967a04>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mword_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Verify the vectors are loaded correctly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-0ffc2d967a04>\u001b[0m in \u001b[0;36mload_vectors\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Read the file and load the vectors, cleaning the words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# The first line contains the number of vectors and their dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnum_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vectorized_weezy_lyrics.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import requests\n",
        "import random\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download NLTK data files (run this only once)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6QzejqeYIbx",
        "outputId": "f3ecc32f-81c7-41d5-a506-7abea11ad5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = 'vectorized_weezy_lyrics.txt'\n",
        "\n",
        "# Function to remove punctuation from words\n",
        "def clean_word(word):\n",
        "    return word.translate(str.maketrans(',', '\"', string.punctuation))\n",
        "\n",
        "# Read the file and load the vectors, cleaning the words\n",
        "def load_vectors(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        # The first line contains the number of vectors and their dimensionality\n",
        "        num_vectors, vector_size = map(int, f.readline().split())\n",
        "        word_vectors = KeyedVectors(vector_size)\n",
        "\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = clean_word(values[0])\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            word_vectors.add_vector(word, vector)\n",
        "\n",
        "    return word_vectors\n",
        "\n",
        "word_vectors = load_vectors(file_path)\n",
        "\n",
        "# Verify the vectors are loaded correctly\n",
        "#print(f\"Loaded {len(word_vectors.key_to_index)} word vectors.\")\n",
        "\n",
        "# Function to fetch rhyming words using Datamuse API\n",
        "def get_rhyming_words(word, num_words=5000):\n",
        "    url = f\"https://api.datamuse.com/words?rel_rhy={word}&max={num_words}\"\n",
        "    response = requests.get(url)\n",
        "    rhymes = response.json()\n",
        "    rhyming_words = [clean_word(rhyme['word']) for rhyme in rhymes if clean_word(rhyme['word']) in word_vectors.key_to_index]\n",
        "    return rhyming_words\n",
        "\n",
        "# Function to generate a grammatically coherent line\n",
        "def generate_line(subject, verb, obj):\n",
        "    return f\"{subject} {verb} {obj}\"\n",
        "\n",
        "# Function to get a word with a specific POS tag\n",
        "def get_word_with_pos(word, pos_tag, used_words):\n",
        "    similar_words = word_vectors.most_similar(word, topn=7000)\n",
        "    for similar_word, _ in similar_words:\n",
        "        cleaned_word = clean_word(similar_word)\n",
        "        if nltk.pos_tag([cleaned_word])[0][1] == pos_tag and cleaned_word not in used_words:\n",
        "            return cleaned_word\n",
        "    # Fallback to any new similar word if specific POS is not found\n",
        "    for similar_word, _ in similar_words:\n",
        "        cleaned_word = clean_word(similar_word)\n",
        "        if cleaned_word not in used_words:\n",
        "            return cleaned_word\n",
        "    return word  # Return the original word if no suitable word is found\n",
        "\n",
        "def generate_verse(start_word, num_lines=30):\n",
        "    verse = []\n",
        "    current_word = clean_word(start_word)\n",
        "    used_words = set()  # To keep track of all used words\n",
        "\n",
        "    for _ in range(num_lines):\n",
        "        # Generate subject, verb, and object\n",
        "        subject = get_word_with_pos(current_word, 'NN', used_words)\n",
        "        verb = get_word_with_pos(subject, 'VB', used_words)\n",
        "        obj = get_word_with_pos(verb, 'NN', used_words)\n",
        "\n",
        "\n",
        "        # Ensure all words are cleaned\n",
        "        subject, verb, obj = clean_word(subject), clean_word(verb), clean_word(obj)\n",
        "\n",
        "        # Generate a grammatically coherent line\n",
        "        line = generate_line(subject, verb, obj)\n",
        "\n",
        "        # Find a rhyming word for the object\n",
        "        rhyming_words = get_rhyming_words(obj)\n",
        "        if rhyming_words:\n",
        "            rhyming_word = random.choice(rhyming_words)\n",
        "            line += f\" with {rhyming_word}\"\n",
        "            used_words.add(rhyming_word)  # Add the rhyming word to the used set\n",
        "        else:\n",
        "            rhyming_word = None\n",
        "\n",
        "        verse.append(line)\n",
        "        current_word = obj  # Update the current word for the next iteration\n",
        "        used_words.update([subject, verb, obj])  # Add words to the used set\n",
        "\n",
        "    return \"\\n\".join(verse)\n",
        "\n",
        "start_word = \"love\"\n",
        "verse = generate_verse(start_word)\n",
        "print(verse)\n",
        "\n",
        "# Save the verse to a file to be used by ChucK\n",
        "with open(\"generated_verse.txt\", \"w\") as f:\n",
        "    f.write(verse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csuWntlkZU6Q",
        "outputId": "b582f9fe-e691-408a-e890-f710920183b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " be \n",
            "’ see ’\n",
            "t go nt\n",
            "niggas know niggas\n",
            "m do m with em\n",
            "s get bitch with switch\n",
            "Cause say love with shove\n",
            "tell make shit with wit\n",
            "put have ve with goatee\n",
            "nigga Tell right with tonight\n",
            "fuck give fuck with stuck\n",
            "world let wan with gone\n",
            "time come pussy\n",
            "need find ll\n",
            "Money Do Money with honey\n",
            "Young See ass with bass\n",
            "look keep money with funny\n",
            "re take die with buy\n",
            "ai Go boy with enjoy\n",
            "Yeah Call yall\n",
            "girl leave motherfucker with fucker\n",
            "want Take turn with burn\n",
            "Carter run whoa with wo\n",
            "Mr Look Mr with sister\n",
            "Hey Believe man with wingspan\n",
            "today believe ball with wall\n",
            "lot Give Nigga\n",
            "head Come head with bread\n",
            "call Make house with mouse\n",
            "Weezy Get Weezy\n"
          ]
        }
      ]
    }
  ]
}